#### Neural Networks
结构:
```mermaid
graph LR
输入层 ---> 隐藏层 ---> 输出层
```

隐藏层$layer_j$层的第$i$个神经元模型:
$$
a_i = g(\vec w_i^{(j)} \cdot \vec x + b_i^{(j)})
$$

其中$g(z)$函数是激活函数，在后面有四种activate function的展示。

##### *Activation Function*:
$g(z)$是activation function，展示了四种在这一段，分别是sigmoid，Relu和linear activation function。

**sigmoid function**是：
$$
g(z) = \frac{1}{1 + e^{(-z)}}
$$
如果执行的任务是多分类任务，那么可以使用 **SoftMax**。

如果执行的任务不是二分类任务,那么更换activation function为**Relu**:
$$
g(z) = \max(0, z)
$$
这个函数小于0的时候输出的结果都为0，当大于0的时候是$g(z) = z$，由于在某些情况下结果不理想，所以有改进的Relu。

或者是**linear activation function**:
$$
g(z) = z
$$
如果不使用激活函数，那神经网络和线性回归模型本质上就没有什么区别，都只能处理线性的问题。对于非线性这种更加困难的问题无法解决。

<br />

---

#### 优化方法

##### Adam Agorithm ---- not just one alpha
如果一个参数朝着一个方向走那就增加$\alpha$，如果来回摆动那就减小$\alpha$