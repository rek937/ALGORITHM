#### Neural Networks
结构:
```mermaid
graph LR
输入层 ---> 隐藏层 ---> 输出层
```

隐藏层$layer_j$层的第$i$个神经元模型:
$$
a_i = g(\vec w_i^{(j)} \cdot \vec x + b_i^{(j)})
$$

其中$g(z)$函数是激活函数，在后面有四种activate function的展示。

##### *Activation Function*:
$g(z)$是activation function，展示了四种在这一段，分别是sigmoid，Relu和linear activation function。

sigmoid function是：
$$
g(z) = \frac{1}{1 + e^{(-z)}}
$$
如果执行的任务不是二分类任务,那么更换activation function为Relu:
$$
g(z) = \max(0, z)
$$

这个函数小于0的时候输出的结果都为0，当大于0的时候是$g(z) = z$，由于在某些情况下结果不理想，所以有改进的Relu。

或者是linear activation function:
$$
g(z) = z
$$
如果不使用激活函数，那神经网络和线性回归模型本质上就没有什么区别，都只能处理线性的问题。对于非线性这种更加困难的问题无法解决。
