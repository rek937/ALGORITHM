<!-- encoding = utf8 -->

#### Logistic Regression Model
和线性回归一样的，最终都要拟合一条直线解决分类问题。区别是逻辑回归完成的是分类任务，用于划分样本的种类，所以选择的 **激活函数（或者）** 和线性函数不一样，逻辑回归模型选择的是sigmoid函数。

sigmoid function:
$$
g(z) = \frac{1}{1 + e^{-z}}, 0 < g(z) < 1
$$

当$g(z)$中的z部分被看作是$f_{{\vec w},{b}}$,那么逻辑回归模型：
$$
f_{{\vec w},{b}}(\vec x) = \frac{1}{1 + e^{-(\vec w \cdot \vec x + b)}}
$$

##### *Decision Boundary*：
因为sigmoid函数是从0到1都会取到数值，所以当它得出小数（也就是取值在0到1之间）的时候，通过设置阈值来划定该预测点的类别。比如二分类问题中小于阈值0.5为0类别， 大于0.5阈值为1类别。

##### *Cost Function*:
从线性回归中推导出来的cost function:
$$
J(\vec{w},b) = \frac{1}{2m}\sum_1^m(f_{\vec{w},b}(x^{(i)}) - y^{(i)})^2
$$
接着可以再把括号中的部分抽取出来构成一个新的function，loss function:
$$
L(f_{{\vec w},{b}}(\vec x^{(i)}), y^{(i)}) = \frac{1}{2}(f_{\vec{w},b}(x^{(i)}) - y^{(i)})^2
$$
其中Loss function的定义为：
$$
L(f_{{\vec w},{b}}(\vec x^{(i)}), y^{(i)}) = \begin{cases}
    - \log {f_{{\vec w},{b}}(\vec x^{(i)})} & \text , y^{(i)} = 1 \\
    - \log{(1 - {f_{{\vec w},{b}}(\vec x^{(i)})})} & \text ,y{(i)} = 0
\end{cases}
$$
为了让式子看起来简单一点所以有了Simplified loss function:
$$
L = -y^{(i)}\log {f_{{\vec w},{b}}(\vec x^{(i)})}-(1-y^{(i)})\log {(1-{f_{{\vec w},{b}}(\vec x^{(i)})})}
$$
所以最终代价函数为：
$$
J(\vec{w},b) = -\frac{1}{m}\sum_1^m[y^{(i)}\log {f_{{\vec w},{b}}(\vec x^{(i)})}-(1-y^{(i)})\log {(1-{f_{{\vec w},{b}}(\vec x^{(i)})})}]
$$

##### *Gredient descent*:
最常见的梯度更新公式，偏导项的更新方式看起来和线性回归模型相似，因为只是改变了一个基础的模型底座，但是在自变量这些没有变化，在偏导的时候也不会发生变化。根据数学推导可以得出来权重和bias的更新公式：
$$
w_j := w - \frac{\alpha}{m}\sum_1^m(f_{\vec{w},b}(x^{(i)})-y^{(i)})x_j^{(i)}
$$
$$
b := w - \frac{\alpha}{m}\sum_1^m(f_{\vec{w},b}(x^{(i)})-y^{(i)})
$$

<br />

----

#### Softmax Regression
假设有$n$个输出，那么softmax regression可以表示为：
$$

$$

<br />

---

#### Over Fitting
模型可以完全拟合训练集上面的所有点，无法完成预测任务。高偏差和高方差的模型都无法使用。
1. **增加训练集的规模**
2. **减少样本特征数量**：控制样本数量和样本规模的比例；手动或者算法（正则化）选择对样本重要的特征
3. **Regularization**

<br />

---

#### Regularization
假设有cost function增加惩罚项之后：
$$
J(\vec{w},b) = \frac{1}{2m}\sum_1^m(f_{\vec{w},b}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_1^j(w_j^{2})
$$
其中$\frac{\lambda}{2m}\sum_1^j(w_j^{2})$就是加入的惩罚项，加入这一项在进行梯度下降的时候会控制权重参数，防止模型过拟合。

##### *正则化线性回归*：
根据正则化的cost function代入线性回归模型中：
$$
J(\vec{w},b) = \frac{1}{2m}\sum_1^m(f_{\vec{w},b}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_1^j(w_j^{2})
$$
之后可以通过梯度下降的线性回归模型得到权重和bias更新公式：
$$
w_j := w - \frac{\alpha}{m}[\sum_1^m(f_{\vec{w},b}(\vec x^{(i)})-y^{(i)})x_j^{(i)} + \lambda w_j]
$$
$$
b := w - \frac{\alpha}{m}\sum_1^m(f_{\vec{w},b}(\vec x^{(i)})-y^{(i)})
$$

##### *正则化逻辑回归*：
根据正则化的cost function代入逻辑回归模型中：
$$
J(\vec{w},b) = -\frac{1}{m}\sum_1^m[y^{(i)}\log {f_{{\vec w},{b}}(\vec x^{(i)})}-(1-y^{(i)})\log {(1-{f_{{\vec w},{b}}(\vec x^{(i)})})}] + \frac{\lambda}{2m}\sum_1^j(w_j^{2})
$$
之后可以通过梯度下降的逻辑回归模型得到权重和bias更新公式：
$$
w_j := w - \frac{\alpha}{m}[\sum_1^m(f_{\vec{w},b}(\vec x^{(i)})-y^{(i)})x_j^{(i)} + \lambda w_j]
$$
$$
b := w - \frac{\alpha}{m}\sum_1^m(f_{\vec{w},b}(\vec x^{(i)})-y^{(i)})
$$


